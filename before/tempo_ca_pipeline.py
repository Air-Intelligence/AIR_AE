#!/usr/bin/env python3
"""
TEMPO L3 California Data Pipeline
==================================
NASA TEMPO L3 ë°ì´í„°(NOâ‚‚/HCHO/Oâ‚ƒ/Cloud)ë¥¼ ìº˜ë¦¬í¬ë‹ˆì•„ ì˜ì—­ì— ëŒ€í•´
ìµœì†Œ ì‹œê°„Â·ìš©ëŸ‰ìœ¼ë¡œ ìˆ˜ì§‘/ì „ì²˜ë¦¬/ë³‘í•©í•˜ëŠ” í†µí•© íŒŒì´í”„ë¼ì¸

ì˜ˆìƒ ì†Œìš”ì‹œê°„:
- í…ŒìŠ¤íŠ¸(4ì£¼, 3ë³€ìˆ˜): ~30ë¶„
- ì „ì²´(3ê°œì›”, 4ë³€ìˆ˜): ~2-3ì‹œê°„ (THREADS=8 ê¸°ì¤€)

ì˜ˆìƒ ìš©ëŸ‰:
- ì›ë³¸ netCDF: ~5-10GB
- ìµœì¢… Parquet: ~200-500MB
"""

import earthaccess
import xarray as xr
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# ============================================================================
# CONFIGURATION - ì—¬ê¸°ë§Œ ìˆ˜ì •í•˜ì„¸ìš”
# ============================================================================

CONFIG = {
    # ê³µê°„ ë²”ìœ„: ìº˜ë¦¬í¬ë‹ˆì•„ (West, South, East, North)
    'BBOX': (-122.5, 37.2, -121.5, 38.0),  # Bay Area (SF, Oakland, San Jose)

    # ì‹œê°„ ë²”ìœ„: 2~3ê°œì›” (ì²˜ìŒì—” 4ì£¼ë¡œ í…ŒìŠ¤íŠ¸ ê¶Œìž¥)
    'DATE_RANGE': ('2023-08-01', '2023-10-31'),  # 3ê°œì›” (2023ë…„ ì—¬ë¦„~ê°€ì„)
    #'DATE_RANGE': ('2024-06-01', '2024-08-31'),  # 3ê°œì›”
    #'DATE_RANGE': ('2024-06-01', '2024-06-28'),  # 4ì£¼ (í…ŒìŠ¤íŠ¸ìš©)

    # ë‹¤ìš´ë¡œë“œ ë³‘ë ¬ ìŠ¤ë ˆë“œ (8~12 ê¶Œìž¥, VM ì½”ì–´ ìˆ˜ì— ë§žê²Œ ì¡°ì •)
    'THREADS': 12,

    # ì¶œë ¥ ë””ë ‰í† ë¦¬
    'OUT_DIR': './tempo_l3_bayarea_202308-202310',

    # ì‹œê°„ ë¦¬ìƒ˜í”Œë§ ('3H' ë˜ëŠ” None)
    'RESAMPLE': '3H',

    # ë‹¤ìš´ë¡œë“œí•  ë³€ìˆ˜ (ì²˜ìŒì—” 2~3ê°œë¡œ í…ŒìŠ¤íŠ¸)
    'VARS': ['NO2', 'HCHO', 'O3', 'CLOUD'],  # ì „ì²´ 4ë³€ìˆ˜
    # 'VARS': ['NO2', 'O3', 'CLOUD'],  # 3ë³€ìˆ˜

    # TEMPO ì œí’ˆ ë²„ì „ (V03 ë˜ëŠ” V04, V04ê°€ ìµœì‹ )
    # 'VERSION': 'V04',
    'VERSION': 'V03',
}

# TEMPO L3 ì»¬ë ‰ì…˜ ì •ì˜ (CSV ê¸°ë°˜)
TEMPO_L3_COLLECTIONS = {
    'NO2': {
        'V03': 'TEMPO_NO2_L3',
        'V04': 'TEMPO_NO2_L3',
    },
    'HCHO': {
        'V03': 'TEMPO_HCHO_L3',
        'V04': 'TEMPO_HCHO_L3',
    },
    'O3': {
        'V03': 'TEMPO_O3TOT_L3',
        'V04': 'TEMPO_O3TOT_L3',
    },
    'CLOUD': {
        'V03': 'TEMPO_CLDO4_L3',
        'V04': 'TEMPO_CLDO4_L3',
    }
}

# ë³€ìˆ˜ëª… ë§¤í•‘ (íŒŒì¼ ë‚´ ì‹¤ì œ í‚¤ â†’ í‘œì¤€ëª…)
# ìš°ì„ ìˆœìœ„ ìˆœìœ¼ë¡œ ì‹œë„
VARIABLE_MAPPINGS = {
    'NO2': [
        'vertical_column_troposphere',
        'tropospheric_vertical_column',
        'nitrogen_dioxide_tropospheric_column',
        'NO2_column',
    ],
    'HCHO': [
        'vertical_column',
        'formaldehyde_vertical_column',
        'HCHO_column',
    ],
    'O3': [
        'vertical_column',
        'ozone_total_vertical_column',
        'O3_column',
        'total_ozone_column',
    ],
    'CLOUD': [
        'cloud_fraction',
        'effective_cloud_fraction',
        'cloud_fraction_o2o2',
    ]
}


# ============================================================================
# CORE FUNCTIONS
# ============================================================================

def authenticate():
    """NASA Earthdata ì¸ì¦"""
    print("ðŸ” NASA Earthdata ë¡œê·¸ì¸ ì¤‘...")
    print("   (ì²˜ìŒ ì‹¤í–‰ ì‹œ ê³„ì • ì •ë³´ ìž…ë ¥ í•„ìš”: https://urs.earthdata.nasa.gov/)")

    auth = earthaccess.login()
    if not auth:
        raise RuntimeError("âŒ ì¸ì¦ ì‹¤íŒ¨! EDL ê³„ì • í™•ì¸ í•„ìš”")

    print("âœ… ì¸ì¦ ì„±ê³µ!\n")
    return auth


def search_granules(var_name, config):
    """CMRì—ì„œ granule ê²€ìƒ‰"""
    short_name = TEMPO_L3_COLLECTIONS[var_name][config['VERSION']]
    version = config['VERSION']

    print(f"ðŸ” ê²€ìƒ‰ ì¤‘: {var_name} ({short_name} {version})")
    print(f"   ê¸°ê°„: {config['DATE_RANGE'][0]} ~ {config['DATE_RANGE'][1]}")
    print(f"   ì˜ì—­: {config['BBOX']}")

    try:
        granules = earthaccess.search_data(
            short_name=short_name,
            version=version,
            temporal=config['DATE_RANGE'],
            bounding_box=config['BBOX'],
        )

        print(f"   â†’ ë°œê²¬: {len(granules)}ê°œ íŒŒì¼\n")
        return granules

    except Exception as e:
        print(f"   âš ï¸  ê²€ìƒ‰ ì‹¤íŒ¨: {e}\n")
        return []


def download_granules(granules, var_name, config):
    """ë³‘ë ¬ ë‹¤ìš´ë¡œë“œ with retry"""
    if not granules:
        return []

    out_dir = Path(config['OUT_DIR']) / 'raw' / var_name
    out_dir.mkdir(parents=True, exist_ok=True)

    print(f"â¬‡ï¸  ë‹¤ìš´ë¡œë“œ ì‹œìž‘: {len(granules)}ê°œ íŒŒì¼ (THREADS={config['THREADS']})")
    print(f"   ì €ìž¥ ê²½ë¡œ: {out_dir}")

    try:
        files = earthaccess.download(
            granules,
            str(out_dir),
            threads=config['THREADS']
        )

        print(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {len(files)}ê°œ íŒŒì¼\n")
        return files

    except Exception as e:
        print(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        print("   â†’ Retry ì‹œë„ ì¤‘ (threads=4)...\n")

        try:
            files = earthaccess.download(
                granules,
                str(out_dir),
                threads=4  # ìŠ¤ë ˆë“œ ì¤„ì—¬ì„œ ìž¬ì‹œë„
            )
            print(f"âœ… Retry ì„±ê³µ: {len(files)}ê°œ íŒŒì¼\n")
            return files
        except:
            print(f"âŒ Retry ì‹¤íŒ¨. ìˆ˜ë™ í™•ì¸ í•„ìš”.\n")
            return []


def find_variable_name(ds, var_type):
    """xarray Datasetì—ì„œ ì‹¤ì œ ë³€ìˆ˜ëª… ìžë™ íƒìƒ‰"""
    candidates = VARIABLE_MAPPINGS[var_type]

    for name in candidates:
        if name in ds.data_vars:
            return name

    # í›„ë³´ì— ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ data_var ë°˜í™˜
    if len(ds.data_vars) > 0:
        fallback = list(ds.data_vars.keys())[0]
        print(f"   âš ï¸  í‘œì¤€ ë³€ìˆ˜ëª… ì—†ìŒ. ëŒ€ì²´: {fallback}")
        return fallback

    raise ValueError(f"ë³€ìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {list(ds.data_vars.keys())}")


def subset_and_convert(files, var_type, config):
    """BBOX ì„œë¸Œì…‹ + Tidy ë³€í™˜"""
    if not files:
        return pd.DataFrame()

    print(f"ðŸ“¦ ì „ì²˜ë¦¬ ì¤‘: {var_type} ({len(files)}ê°œ íŒŒì¼)")

    dfs = []
    w, s, e, n = config['BBOX']

    for i, fpath in enumerate(files, 1):
        try:
            # netCDF ì—´ê¸°
            ds = xr.open_dataset(fpath, decode_times=True)

            # ì²« íŒŒì¼ì—ì„œ ë³€ìˆ˜ëª… í™•ì¸
            if i == 1:
                var_name = find_variable_name(ds, var_type)
                print(f"   ë³€ìˆ˜ëª…: {var_name}")

            # BBOX ì„œë¸Œì…‹ (lat/lon ì¢Œí‘œëª… ìžë™ íƒìƒ‰)
            lat_key = 'latitude' if 'latitude' in ds.coords else 'lat'
            lon_key = 'longitude' if 'longitude' in ds.coords else 'lon'

            ds_subset = ds.where(
                (ds[lat_key] >= s) & (ds[lat_key] <= n) &
                (ds[lon_key] >= w) & (ds[lon_key] <= e),
                drop=True
            )

            # Tidy ë³€í™˜
            df = ds_subset[[var_name]].to_dataframe().reset_index()

            # ì»¬ëŸ¼ëª… í‘œì¤€í™”
            df = df.rename(columns={
                lat_key: 'lat',
                lon_key: 'lon',
                var_name: var_type.lower()
            })

            # í•„ìˆ˜ ì»¬ëŸ¼ë§Œ ì„ íƒ
            df = df[['time', 'lat', 'lon', var_type.lower()]]

            # ê²°ì¸¡ì¹˜/ì´ìƒì¹˜ ì œê±°
            df = df.dropna()

            dfs.append(df)
            ds.close()

            if i % 10 == 0:
                print(f"   ì§„í–‰: {i}/{len(files)}...")

        except Exception as e:
            print(f"   âš ï¸  íŒŒì¼ {i} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            continue

    if not dfs:
        print(f"   âŒ ì²˜ë¦¬ ê°€ëŠ¥í•œ íŒŒì¼ ì—†ìŒ\n")
        return pd.DataFrame()

    # ì „ì²´ ë³‘í•©
    df_all = pd.concat(dfs, ignore_index=True)
    print(f"   â†’ ì´ {len(df_all):,}í–‰ ìƒì„±")
    print(f"   â†’ ì‹œê°„ ë²”ìœ„: {df_all['time'].min()} ~ {df_all['time'].max()}\n")

    return df_all


def resample_temporal(df, freq='3H'):
    """ì‹œê°„ ë¦¬ìƒ˜í”Œë§ (ì„ íƒì‚¬í•­)"""
    if freq is None or df.empty:
        return df

    print(f"â° ì‹œê°„ ë¦¬ìƒ˜í”Œë§: {freq}")

    df = df.set_index('time')
    df_resampled = df.groupby(['lat', 'lon']).resample(freq).mean().reset_index()

    print(f"   â†’ {len(df_resampled):,}í–‰\n")
    return df_resampled


def merge_all_variables(dfs_dict, config):
    """ëª¨ë“  ë³€ìˆ˜ë¥¼ ì‹œê³µê°„ ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©"""
    print("ðŸ”— ë³€ìˆ˜ ë³‘í•© ì¤‘...")

    # ì²« ë²ˆì§¸ ë³€ìˆ˜ë¥¼ baseë¡œ ì‹œìž‘
    var_list = list(dfs_dict.keys())
    if not var_list:
        raise ValueError("ë³‘í•©í•  ë°ì´í„° ì—†ìŒ")

    merged = dfs_dict[var_list[0]].copy()
    print(f"   Base: {var_list[0]} ({len(merged):,}í–‰)")

    # ë‚˜ë¨¸ì§€ ë³€ìˆ˜ ìˆœì°¨ ì¡°ì¸
    for var in var_list[1:]:
        df = dfs_dict[var]

        before = len(merged)
        merged = merged.merge(
            df,
            on=['time', 'lat', 'lon'],
            how='inner'  # ëª¨ë“  ë³€ìˆ˜ê°€ ìžˆëŠ” ì‹œê³µê°„ë§Œ ìœ ì§€
        )
        after = len(merged)

        print(f"   + {var}: {before:,} â†’ {after:,}í–‰")

    print(f"\nâœ… ìµœì¢…: {len(merged):,}í–‰ Ã— {len(merged.columns)}ì—´")
    print(f"   ì»¬ëŸ¼: {list(merged.columns)}\n")

    return merged


def save_outputs(df, config):
    """Parquet + CSV ìƒ˜í”Œ ì €ìž¥"""
    if df.empty:
        print("âš ï¸  ì €ìž¥í•  ë°ì´í„° ì—†ìŒ\n")
        return

    out_dir = Path(config['OUT_DIR'])
    out_dir.mkdir(parents=True, exist_ok=True)

    # Parquet (ì „ì²´)
    parquet_path = out_dir / 'tempo_l3_ca_merged.parquet'
    print(f"ðŸ’¾ Parquet ì €ìž¥ ì¤‘: {parquet_path}")
    df.to_parquet(parquet_path, compression='snappy', index=False)

    size_mb = parquet_path.stat().st_size / 1024 / 1024
    print(f"   â†’ {size_mb:.1f} MB\n")

    # CSV ìƒ˜í”Œ (1ë§Œí–‰)
    csv_path = out_dir / 'tempo_l3_ca_sample.csv'
    sample_size = min(10000, len(df))
    print(f"ðŸ’¾ CSV ìƒ˜í”Œ ì €ìž¥ ì¤‘: {csv_path} ({sample_size:,}í–‰)")
    df.head(sample_size).to_csv(csv_path, index=False)
    print()


# ============================================================================
# MAIN PIPELINE
# ============================================================================

def main():
    print("="*70)
    print("  TEMPO L3 California Data Pipeline")
    print("="*70)
    print(f"ì„¤ì •:")
    print(f"  - BBOX: {CONFIG['BBOX']}")
    print(f"  - ê¸°ê°„: {CONFIG['DATE_RANGE']}")
    print(f"  - ë³€ìˆ˜: {CONFIG['VARS']}")
    print(f"  - ë²„ì „: {CONFIG['VERSION']}")
    print(f"  - ìŠ¤ë ˆë“œ: {CONFIG['THREADS']}")
    print(f"  - ë¦¬ìƒ˜í”Œ: {CONFIG['RESAMPLE']}")
    print(f"  - ì¶œë ¥: {CONFIG['OUT_DIR']}")
    print("="*70)
    print()

    start_time = datetime.now()

    # 1. ì¸ì¦
    authenticate()

    # 2. ê° ë³€ìˆ˜ë³„ íŒŒì´í”„ë¼ì¸
    dfs = {}

    for var in CONFIG['VARS']:
        print(f"\n{'#'*70}")
        print(f"#  {var} ì²˜ë¦¬ ì‹œìž‘")
        print(f"{'#'*70}\n")

        # 2.1 ê²€ìƒ‰
        granules = search_granules(var, CONFIG)
        if not granules:
            print(f"âš ï¸  {var}: granule ì—†ìŒ. ìŠ¤í‚µ.\n")
            continue

        # 2.2 ë‹¤ìš´ë¡œë“œ
        files = download_granules(granules, var, CONFIG)
        if not files:
            print(f"âš ï¸  {var}: ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨. ìŠ¤í‚µ.\n")
            continue

        # 2.3 ì„œë¸Œì…‹ + Tidy ë³€í™˜
        df = subset_and_convert(files, var, CONFIG)
        if df.empty:
            print(f"âš ï¸  {var}: ì „ì²˜ë¦¬ ì‹¤íŒ¨. ìŠ¤í‚µ.\n")
            continue

        # 2.4 ì‹œê°„ ë¦¬ìƒ˜í”Œ
        df = resample_temporal(df, CONFIG['RESAMPLE'])

        dfs[var] = df

    # 3. ë³€ìˆ˜ ë³‘í•©
    if not dfs:
        print("\nâŒ ì²˜ë¦¬ëœ ë³€ìˆ˜ ì—†ìŒ. ì¢…ë£Œ.\n")
        return

    print(f"\n{'='*70}")
    print("  ë³€ìˆ˜ ë³‘í•©")
    print(f"{'='*70}\n")

    merged = merge_all_variables(dfs, CONFIG)

    # 4. ì €ìž¥
    print(f"{'='*70}")
    print("  ìµœì¢… ì €ìž¥")
    print(f"{'='*70}\n")

    save_outputs(merged, CONFIG)

    # 5. í†µê³„ ìš”ì•½
    elapsed = (datetime.now() - start_time).total_seconds()
    print(f"{'='*70}")
    print("  ì™„ë£Œ!")
    print(f"{'='*70}")
    print(f"ì†Œìš”ì‹œê°„: {elapsed/60:.1f}ë¶„")
    print(f"ìµœì¢… ë°ì´í„°: {len(merged):,}í–‰ Ã— {len(merged.columns)}ì—´")
    print(f"ì¶œë ¥: {CONFIG['OUT_DIR']}/tempo_l3_ca_merged.parquet")
    print(f"{'='*70}\n")

    # 6. ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
    print("ðŸ“Š ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:")
    print(merged.head(10))
    print()
    print("ðŸ“ˆ ê¸°ì´ˆ í†µê³„:")
    print(merged.describe())


if __name__ == '__main__':
    main()
